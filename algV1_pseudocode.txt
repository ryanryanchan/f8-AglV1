3D Resource management algorithm 

- loop: display what I can at X,Y,Z every frame
	- if you get the same X Y Z all the time, at some point all the models should be downloaded and visible

- output a sorted list of things to download
- keep as much data in cache as possible
- you get a list of all instances from the cloud first, then you can figure out which models are necessary
- you get total amount availiable, total used per model
- making a ranking system based on 
	- data size
	- model size
	- distance from user
	- number of instances (within range)


algorithmV1:
	- prioritize physically biggest item
	- smallest thing would get filled in last

-------------------------------- algV1 pseudo-code --------------------------------

given:
	projectID
	latlng

global variabes: (?)
	gpu_memory
	cpu_memory
	cache_memory

current memory:
	gpu_data[]
	cpu_data[]
	cache_data[]

things to download:
	cloud_to_cpu[]
	cache_to_cpu[]
	cpu_to_cache[]
	cpu_to_gpu[]

things to delete:
	cpu_delete[]
	gpu_delete[]
	cache_delete[]

starting_list = get instances within distance(X,Y,Z)

// sort this list by importance
// this list can change depending on different factors later on
starting_list.sort_modelSize()

// **usage is defined as every time a model is used in a frame

// loop through list
for(x in starting_list)
	// if it's already in the gpu we're good
	if x in gpu_data
		starting_list.remove(x)
		x.increment_usage()
	
	// if it's not in the gpu but in the cpu, load to gpu
	else if x in cpu_data:
		starting_list.remove(x)
		cpu_to_gpu.append(x)
	
	// if it's not in gpu, cpu, check cache
	else if x in cache_data:
		starting_list.remove(x)
		cache_to_cpu.append(x)
		x.increment_cache_load()

// whatever is left in starting list is not in system at all. we need to download into cpu
cloud_to_cpu = starting_list


// check if memory is too full
// clear room so everything that's needed fits

// not sure if we need to add gpu_data + cpu_gpu
gpu_data.sort_by_usage()
while(gpu_data + cpu_to_gpu >= 95%)	
	x = gpu_data.pop()
	gpu_delete.append(x)

cpu_data.sort_by_usage()
while(cpu_data + cache_to_cpu + cloud_to_cpu >= 95%)
	x = cpu_data.pop()
	cpu_delete.append(x)
	cache_data.append(x)

// cache sort by usage? 
cache_data.sort_by_something()
while(cache_data + cpu_to_cache >= 95%)
	x = cache.pop()
	cache_delete.append(x)